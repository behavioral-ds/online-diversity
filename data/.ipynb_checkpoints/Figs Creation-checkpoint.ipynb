{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f27336f14abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatheffects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPathEffects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure_factory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import json\n",
    "import pickle\n",
    "import io\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import math \n",
    "import powerlaw\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reddit Data\n",
    "df_reddit = pd.read_csv('sourceFiles/reddit.csv', sep = ' ')\n",
    "df_reddit_plot = df_reddit[['Domain']]\n",
    "df_reddit.set_index('Domain', inplace = True)\n",
    "\n",
    "reddit_year_list = ['2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019', 'All']\n",
    "top10_list = []\n",
    "top50_list = []\n",
    "top100_list = []\n",
    "top500_list = []\n",
    "top1000_list = []\n",
    "for year in reddit_year_list:\n",
    "    if year != 'All':\n",
    "        df_reddit_plot[year] = df_reddit.loc[:,df_reddit.columns.str.contains(year)].sum(axis = 1).tolist()\n",
    "        top10_list.append(df_reddit_plot[year].sort_values(ascending = False).head(10).sum()/df_reddit_plot[year].sum())\n",
    "        top50_list.append(df_reddit_plot[year].sort_values(ascending = False).head(50).sum()/df_reddit_plot[year].sum())\n",
    "        top100_list.append(df_reddit_plot[year].sort_values(ascending = False).head(100).sum()/df_reddit_plot[year].sum())\n",
    "        top500_list.append(df_reddit_plot[year].sort_values(ascending = False).head(500).sum()/df_reddit_plot[year].sum())\n",
    "        top1000_list.append(df_reddit_plot[year].sort_values(ascending = False).head(1000).sum()/df_reddit_plot[year].sum())\n",
    "    else:\n",
    "        df_reddit_plot[year] = df_reddit.loc[:,df_reddit.columns.str.contains('|'.join(reddit_year_list[:-1]))].sum(axis = 1).tolist()\n",
    "        \n",
    "df_reddit_top_percent_trend = pd.DataFrame([reddit_year_list[:-1],top10_list,top50_list,top100_list, top500_list, top1000_list]).transpose()\n",
    "df_reddit_top_percent_trend.columns = ['Year','top10','top50','top100','top500','top1000']\n",
    "df_reddit_top_percent_trend.set_index('Year', inplace = True)\n",
    "df_reddit_top_percent_trend = df_reddit_top_percent_trend.apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reddit Data\n",
    "df_twitter = pd.read_csv('sourceFiles/twitter_new.csv')\n",
    "df_twitter_plot = df_twitter[['Domain']]\n",
    "df_twitter.set_index('Domain', inplace = True)\n",
    "\n",
    "twitter_year_list = ['2011','2012','2013','2014','2015','2016','2017','2018','2019', 'All']\n",
    "top10_list = []\n",
    "top50_list = []\n",
    "top100_list = []\n",
    "top500_list = []\n",
    "top1000_list = []\n",
    "for year in twitter_year_list:\n",
    "    if year != 'All':\n",
    "        df_twitter_plot[year] = df_twitter.loc[:,df_twitter.columns.str.contains(year)].sum(axis = 1).tolist()\n",
    "        top10_list.append(df_twitter_plot[year].sort_values(ascending = False).head(10).sum()/df_twitter_plot[year].sum())\n",
    "        top50_list.append(df_twitter_plot[year].sort_values(ascending = False).head(50).sum()/df_twitter_plot[year].sum())\n",
    "        top100_list.append(df_twitter_plot[year].sort_values(ascending = False).head(100).sum()/df_twitter_plot[year].sum())\n",
    "        top500_list.append(df_twitter_plot[year].sort_values(ascending = False).head(500).sum()/df_twitter_plot[year].sum())\n",
    "        top1000_list.append(df_twitter_plot[year].sort_values(ascending = False).head(1000).sum()/df_twitter_plot[year].sum())\n",
    "    else:\n",
    "        df_twitter_plot[year] = df_twitter.loc[:,df_twitter.columns.str.contains('|'.join(twitter_year_list[:-1]))].sum(axis = 1).tolist()\n",
    "        \n",
    "\n",
    "df_twitter_top_percent_trend = pd.DataFrame([twitter_year_list[:-1],top10_list,top50_list,top100_list, top500_list, top1000_list]).transpose()\n",
    "df_twitter_top_percent_trend.columns = ['Year','top10','top50','top100','top500','top1000']\n",
    "df_twitter_top_percent_trend.set_index('Year', inplace = True)\n",
    "df_twitter_top_percent_trend = df_twitter_top_percent_trend.apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Power Law models for Reddit and Twitter datasets\n",
    "fit_reddit_2006 = powerlaw.Fit(df_reddit_plot['2006'][df_reddit_plot['2006'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2007 = powerlaw.Fit(df_reddit_plot['2007'][df_reddit_plot['2007'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2008 = powerlaw.Fit(df_reddit_plot['2008'][df_reddit_plot['2008'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2009 = powerlaw.Fit(df_reddit_plot['2009'][df_reddit_plot['2009'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2010 = powerlaw.Fit(df_reddit_plot['2010'][df_reddit_plot['2010'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2011 = powerlaw.Fit(df_reddit_plot['2011'][df_reddit_plot['2011'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2012 = powerlaw.Fit(df_reddit_plot['2012'][df_reddit_plot['2012'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2013 = powerlaw.Fit(df_reddit_plot['2013'][df_reddit_plot['2013'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2014 = powerlaw.Fit(df_reddit_plot['2014'][df_reddit_plot['2014'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2015 = powerlaw.Fit(df_reddit_plot['2015'][df_reddit_plot['2015'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2016 = powerlaw.Fit(df_reddit_plot['2016'][df_reddit_plot['2016'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2017 = powerlaw.Fit(df_reddit_plot['2017'][df_reddit_plot['2017'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2018 = powerlaw.Fit(df_reddit_plot['2018'][df_reddit_plot['2018'] > 0], fit_method = 'KS')\n",
    "fit_reddit_2019 = powerlaw.Fit(df_reddit_plot['2019'][df_reddit_plot['2019'] > 0], fit_method = 'KS')\n",
    "fit_reddit_all = powerlaw.Fit(df_reddit_plot['All'][df_reddit_plot['All'] > 0], fit_method = 'KS')\n",
    "\n",
    "fit_twitter_2011 = powerlaw.Fit(df_twitter_plot['2011'][df_twitter_plot['2011'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2012 = powerlaw.Fit(df_twitter_plot['2012'][df_twitter_plot['2012'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2013 = powerlaw.Fit(df_twitter_plot['2013'][df_twitter_plot['2013'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2014 = powerlaw.Fit(df_twitter_plot['2014'][df_twitter_plot['2014'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2015 = powerlaw.Fit(df_twitter_plot['2015'][df_twitter_plot['2015'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2016 = powerlaw.Fit(df_twitter_plot['2016'][df_twitter_plot['2016'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2017 = powerlaw.Fit(df_twitter_plot['2017'][df_twitter_plot['2017'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2018 = powerlaw.Fit(df_twitter_plot['2018'][df_twitter_plot['2018'] > 0], fit_method = 'KS')\n",
    "fit_twitter_2019 = powerlaw.Fit(df_twitter_plot['2019'][df_twitter_plot['2019'] > 0], fit_method = 'KS')\n",
    "fit_twitter_all = powerlaw.Fit(df_twitter_plot['All'][df_twitter_plot['All'] > 0], fit_method = 'KS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CommonCrawl Data\n",
    "df_cc = pd.read_csv('outputFiles/commoncrawl.csv')\n",
    "\n",
    "timeline = df_cc.columns.tolist()[1:]\n",
    "sum_pr_top1m = []\n",
    "sum_pr_10k = []\n",
    "sum_pr_1k = []\n",
    "for i in timeline:\n",
    "    df = df_cc[~df_cc[i].isna()][['Domains',i]].sort_values(i, ascending = False)\n",
    "    sum_pr_top1m.append(df[i].sum())\n",
    "    sum_pr_10k.append(df.head(10000)[i].sum())\n",
    "    sum_pr_1k.append(df.head(1000)[i].sum())\n",
    "    \n",
    "df_pr_trend = pd.DataFrame([timeline,sum_pr_top1m,sum_pr_10k,sum_pr_1k]).transpose()\n",
    "df_pr_trend.columns = ['Year','Top 1m','Top 10k','Top1k']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig2 in paper\n",
    "fig_trend, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(25,5))\n",
    "\n",
    "sns.lineplot(data=df_reddit_top_percent_trend, ax = ax1, palette = ['k','r','lime','b','cyan'], dashes=False)\n",
    "ax1.set_xticks(['2007','2009','2011','2013','2015','2017', '2019'])\n",
    "ax1.set_xlabel('Year', fontdict = {'fontsize': 12})\n",
    "ax1.set_ylabel('Percentage covered in Reddit', fontdict = {'fontsize': 12})\n",
    "ax1.set_title('A - Percentage of all attention received \\n by the most popular domains in Reddit' , \n",
    "              loc = 'left', fontdict = {'fontsize': 14, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True,  ax = ax1)\n",
    "legend1 = ax1.legend(prop={'size': 12})\n",
    "frame1 = legend1.get_frame()\n",
    "frame1.set_facecolor('white')\n",
    "frame1.set_edgecolor('white')\n",
    "\n",
    "\n",
    "sns.lineplot(data=df_twitter_top_percent_trend, ax = ax2, palette = ['k','r','lime','b','cyan'], dashes=False)\n",
    "ax2.set_xticks(['2011','2013','2015','2017', '2019'])\n",
    "ax2.set_xlabel('Year', fontdict = {'fontsize': 12})\n",
    "ax2.set_ylabel('Percentage covered in Twitter', fontdict = {'fontsize': 12})\n",
    "ax2.set_title('B - Percentage of all attention received \\n by the most popular domains in Twitter' , \n",
    "              loc = 'left', fontdict = {'fontsize': 14, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True,  ax = ax2)\n",
    "# legend2 = ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 10})\n",
    "legend2 = ax2.legend(loc='lower right', prop={'size': 12})\n",
    "frame2 = legend2.get_frame()\n",
    "frame2.set_facecolor('white')\n",
    "frame2.set_edgecolor('white')\n",
    "\n",
    "\n",
    "df_pr_trend.set_index('Year').plot(color = ['cyan','blue','lime'], ax = ax3)\n",
    "ax3.set_xlabel('Quarter', fontdict = {'fontsize': 12})\n",
    "ax3.set_ylabel('Percentage of links indexed across entire web', fontdict = {'fontsize': 12})\n",
    "ax3.set_title('C - Percentage of all attention received \\n most linked to domains across entire web' , \n",
    "              loc = 'left', fontdict = {'fontsize': 14, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True,  ax = ax3)\n",
    "legend3 = ax3.legend(frameon = 1, loc = 'center left', prop={'size': 12})\n",
    "frame3 = legend3.get_frame()\n",
    "frame3.set_facecolor('white')\n",
    "frame3.set_edgecolor('white')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig3 in paper\n",
    "fig_ccdf, (ax4, ax5) = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "\n",
    "powerlaw.plot_ccdf(df_reddit_plot['All'][df_reddit_plot['All']>0], color = 'k', label = 'All', linestyle = 'dashed', ax = ax4)\n",
    "powerlaw.plot_ccdf(df_reddit_plot['2006'][df_reddit_plot['2006']>0], color = 'r', label = '2006', ax = ax4)\n",
    "powerlaw.plot_ccdf(df_reddit_plot['2009'][df_reddit_plot['2009']>0], color = 'y', label = '2009', ax = ax4)\n",
    "powerlaw.plot_ccdf(df_reddit_plot['2012'][df_reddit_plot['2012']>0], color = 'lime', label = '2012', ax = ax4)\n",
    "powerlaw.plot_ccdf(df_reddit_plot['2016'][df_reddit_plot['2016']>0], color = 'cyan', label = '2016', ax = ax4)\n",
    "powerlaw.plot_ccdf(df_reddit_plot['2019'][df_reddit_plot['2019']>0], color = 'b', label = '2019', ax = ax4)\n",
    "ax4.set_xticks([1e+00, 1e+02, 1e+04, 1e+06, 1e+08])\n",
    "ax4.set_xlabel('User attention for a domain (number of links)', fontdict = {'fontsize': 9})\n",
    "ax4.set_ylabel('Empirical CCDF', fontdict = {'fontsize': 9})\n",
    "ax4.set_title('A - CCDF of human \\n online attention in Reddit' , loc = 'left', \n",
    "              fontdict = {'fontsize': 10, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True, ax = ax4)\n",
    "legend4 = ax4.legend(prop={'size': 8})\n",
    "frame4 = legend4.get_frame()\n",
    "frame4.set_facecolor('white')\n",
    "frame4.set_edgecolor('white')\n",
    "\n",
    "powerlaw.plot_ccdf(df_twitter_plot['All'][df_twitter_plot['All']>0], color = 'k', label = 'All', linestyle = 'dashed', ax = ax5)\n",
    "powerlaw.plot_ccdf(df_twitter_plot['2012'][df_twitter_plot['2012']>0], color = 'y', label = '2012', ax = ax5)\n",
    "powerlaw.plot_ccdf(df_twitter_plot['2014'][df_twitter_plot['2014']>0], color = 'lime', label = '2014', ax = ax5)\n",
    "powerlaw.plot_ccdf(df_twitter_plot['2016'][df_twitter_plot['2016']>0], color = 'cyan', label = '2016', ax = ax5)\n",
    "powerlaw.plot_ccdf(df_twitter_plot['2019'][df_twitter_plot['2019']>0], color = 'b', label = '2019', ax = ax5)\n",
    "ax5.set_xticks([1e+00, 1e+02, 1e+04, 1e+06, 1e+08])\n",
    "ax5.set_xlabel('User attention for a domain (number of links)', fontdict = {'fontsize': 9})\n",
    "ax5.set_ylabel('Empirical CCDF', fontdict = {'fontsize': 9})\n",
    "ax5.set_title('B - CCDF of human \\n online attention in Twitter' , loc = 'left', \n",
    "              fontdict = {'fontsize': 10, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True, ax = ax5)\n",
    "legend5 = ax5.legend(prop={'size': 8})\n",
    "frame5 = legend5.get_frame()\n",
    "frame5.set_facecolor('white')\n",
    "frame5.set_edgecolor('white')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig9 in paper\n",
    "fig_ccdf_comparison, (ax6, ax7) = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "\n",
    "fit_reddit_2016.plot_ccdf(color = 'k', label = '2016 Empirical CCDF', linewidth=1.8, ax = ax6)\n",
    "fit_reddit_2016.power_law.plot_ccdf(color = 'r', ax = ax6, label = 'PowerLaw Fit', linestyle = 'dashed', linewidth=1.2)\n",
    "fit_reddit_2016.lognormal.plot_ccdf(color = 'g', ax = ax6, label = 'Lognormal Fit', linestyle = 'dashed', linewidth=1.2)\n",
    "fit_reddit_2016.exponential.plot_ccdf(color = 'b', ax = ax6, label = 'Exponential Fit', linestyle = 'dashed', linewidth=1.2)\n",
    "ax6.set_xticks([1e+01, 1e+03, 1e+05, 1e+07])\n",
    "ax6.set_xlabel('User attention for a domain (number of links)', fontdict = {'fontsize': 9})\n",
    "ax6.set_ylabel('CCDF', fontdict = {'fontsize': 9})\n",
    "ax6.set_title('A - CCDFs of empirical \\n and fitted data in Reddit' , loc = 'left', \n",
    "              fontdict = {'fontsize': 10, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True, ax = ax6)\n",
    "legend6 = ax6.legend(loc = 'lower right', prop={'size': 8})\n",
    "frame6 = legend6.get_frame()\n",
    "frame6.set_facecolor('white')\n",
    "frame6.set_edgecolor('white')\n",
    "\n",
    "fit_twitter_2016.plot_ccdf(color = 'k', label = '2016 Empirical CCDF', linewidth=1.8, ax = ax7)\n",
    "fit_twitter_2016.power_law.plot_ccdf(color = 'r', ax = ax7, label = 'PowerLaw Fit', linestyle = 'dashed', linewidth=1.2)\n",
    "fit_twitter_2016.lognormal.plot_ccdf(color = 'g', ax = ax7, label = 'Lognormal Fit', linestyle = 'dashed', linewidth=1.2)\n",
    "fit_twitter_2016.exponential.plot_ccdf(color = 'b', ax = ax7, label = 'Exponential Fit', linestyle = 'dashed', linewidth=1.2)\n",
    "ax7.set_xticks([1e+01, 1e+03, 1e+05, 1e+07, 1e+09])\n",
    "ax7.set_xlabel('User attention for a domain (number of links)', fontdict = {'fontsize': 9})\n",
    "ax7.set_ylabel('CCDF', fontdict = {'fontsize': 9})\n",
    "ax7.set_title('B - CCDFs of empirical \\n and fitted data in Twitter' , loc = 'left', \n",
    "              fontdict = {'fontsize': 10, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True, ax = ax7)\n",
    "legend7 = ax7.legend(loc = 'lower right', prop={'size': 8})\n",
    "frame7 = legend7.get_frame()\n",
    "frame7.set_facecolor('white')\n",
    "frame7.set_edgecolor('white')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig7 in paper\n",
    "df_tsla = pd.read_csv('outputFiles/Tesla_data.csv')\n",
    "fig_tsla = go.Figure()\n",
    "\n",
    "fig_tsla.add_trace(go.Scatter(x=df_tsla['mnth_yr'], y=df_tsla['normalised_tsla_reddit'],\n",
    "                              mode='lines',name='Reddit', line=dict(color='red')))\n",
    "\n",
    "fig_tsla.add_trace(go.Scatter(x=df_tsla['mnth_yr'], y=df_tsla['normalised_tsla_twitter'],\n",
    "                              mode='lines',name='Twitter',line=dict(color='#33ccff')))\n",
    "\n",
    "fig_tsla.add_trace(go.Scatter(x=df_tsla['mnth_yr'], y=df_tsla['normalised_EV'],\n",
    "                              mode='lines',name='EV', line=dict(color='green')))\n",
    "\n",
    "fig_tsla.update_layout(template = 'simple_white')\n",
    "fig_tsla.update_xaxes(showline=True, linecolor='black', mirror=True)\n",
    "fig_tsla.update_yaxes(showline=True, linecolor='black', mirror=True)\n",
    "\n",
    "fig_tsla.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
