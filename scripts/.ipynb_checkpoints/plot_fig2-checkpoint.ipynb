{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] File sourceFiles/reddit.csv does not exist: 'sourceFiles/reddit.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb6858b7282a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load Reddit Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_reddit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sourceFiles/reddit.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf_reddit_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_reddit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Domain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrizoiu/anaconda3/envs/online-diversity/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrizoiu/anaconda3/envs/online-diversity/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrizoiu/anaconda3/envs/online-diversity/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrizoiu/anaconda3/envs/online-diversity/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrizoiu/anaconda3/envs/online-diversity/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] File sourceFiles/reddit.csv does not exist: 'sourceFiles/reddit.csv'"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import powerlaw\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# Load Reddit Data\n",
    "df_reddit = pd.read_csv('sourceFiles/reddit.csv', sep = ' ')\n",
    "df_reddit_plot = df_reddit[['Domain']]\n",
    "df_reddit.set_index('Domain', inplace = True)\n",
    "\n",
    "reddit_year_list = ['2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019', 'All']\n",
    "top10_list = []\n",
    "top50_list = []\n",
    "top100_list = []\n",
    "top500_list = []\n",
    "top1000_list = []\n",
    "for year in reddit_year_list:\n",
    "    if year != 'All':\n",
    "        df_reddit_plot[year] = df_reddit.loc[:,df_reddit.columns.str.contains(year)].sum(axis = 1).tolist()\n",
    "        top10_list.append(df_reddit_plot[year].sort_values(ascending = False).head(10).sum()/df_reddit_plot[year].sum())\n",
    "        top50_list.append(df_reddit_plot[year].sort_values(ascending = False).head(50).sum()/df_reddit_plot[year].sum())\n",
    "        top100_list.append(df_reddit_plot[year].sort_values(ascending = False).head(100).sum()/df_reddit_plot[year].sum())\n",
    "        top500_list.append(df_reddit_plot[year].sort_values(ascending = False).head(500).sum()/df_reddit_plot[year].sum())\n",
    "        top1000_list.append(df_reddit_plot[year].sort_values(ascending = False).head(1000).sum()/df_reddit_plot[year].sum())\n",
    "    else:\n",
    "        df_reddit_plot[year] = df_reddit.loc[:,df_reddit.columns.str.contains('|'.join(reddit_year_list[:-1]))].sum(axis = 1).tolist()\n",
    "        \n",
    "df_reddit_top_percent_trend = pd.DataFrame([reddit_year_list[:-1],top10_list,top50_list,top100_list, top500_list, top1000_list]).transpose()\n",
    "df_reddit_top_percent_trend.columns = ['Year','top10','top50','top100','top500','top1000']\n",
    "df_reddit_top_percent_trend.set_index('Year', inplace = True)\n",
    "df_reddit_top_percent_trend = df_reddit_top_percent_trend.apply(pd.to_numeric)\n",
    "\n",
    "# Load Twitter Data\n",
    "df_twitter = pd.read_csv('sourceFiles/twitter_new.csv')\n",
    "df_twitter_plot = df_twitter[['Domain']]\n",
    "df_twitter.set_index('Domain', inplace = True)\n",
    "\n",
    "twitter_year_list = ['2011','2012','2013','2014','2015','2016','2017','2018','2019', 'All']\n",
    "top10_list = []\n",
    "top50_list = []\n",
    "top100_list = []\n",
    "top500_list = []\n",
    "top1000_list = []\n",
    "for year in twitter_year_list:\n",
    "    if year != 'All':\n",
    "        df_twitter_plot[year] = df_twitter.loc[:,df_twitter.columns.str.contains(year)].sum(axis = 1).tolist()\n",
    "        top10_list.append(df_twitter_plot[year].sort_values(ascending = False).head(10).sum()/df_twitter_plot[year].sum())\n",
    "        top50_list.append(df_twitter_plot[year].sort_values(ascending = False).head(50).sum()/df_twitter_plot[year].sum())\n",
    "        top100_list.append(df_twitter_plot[year].sort_values(ascending = False).head(100).sum()/df_twitter_plot[year].sum())\n",
    "        top500_list.append(df_twitter_plot[year].sort_values(ascending = False).head(500).sum()/df_twitter_plot[year].sum())\n",
    "        top1000_list.append(df_twitter_plot[year].sort_values(ascending = False).head(1000).sum()/df_twitter_plot[year].sum())\n",
    "    else:\n",
    "        df_twitter_plot[year] = df_twitter.loc[:,df_twitter.columns.str.contains('|'.join(twitter_year_list[:-1]))].sum(axis = 1).tolist()\n",
    "        \n",
    "\n",
    "df_twitter_top_percent_trend = pd.DataFrame([twitter_year_list[:-1],top10_list,top50_list,top100_list, top500_list, top1000_list]).transpose()\n",
    "df_twitter_top_percent_trend.columns = ['Year','top10','top50','top100','top500','top1000']\n",
    "df_twitter_top_percent_trend.set_index('Year', inplace = True)\n",
    "df_twitter_top_percent_trend = df_twitter_top_percent_trend.apply(pd.to_numeric)\n",
    "\n",
    "# Load CommonCrawl Data\n",
    "df_cc = pd.read_csv('outputFiles/commoncrawl.csv')\n",
    "\n",
    "timeline = df_cc.columns.tolist()[1:]\n",
    "sum_pr_top1m = []\n",
    "sum_pr_10k = []\n",
    "sum_pr_1k = []\n",
    "for i in timeline:\n",
    "    df = df_cc[~df_cc[i].isna()][['Domains',i]].sort_values(i, ascending = False)\n",
    "    sum_pr_top1m.append(df[i].sum())\n",
    "    sum_pr_10k.append(df.head(10000)[i].sum())\n",
    "    sum_pr_1k.append(df.head(1000)[i].sum())\n",
    "    \n",
    "df_pr_trend = pd.DataFrame([timeline,sum_pr_top1m,sum_pr_10k,sum_pr_1k]).transpose()\n",
    "df_pr_trend.columns = ['Year','Top 1m','Top 10k','Top1k']\n",
    "\n",
    "# Fig2 in paper\n",
    "fig_trend, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(25,5))\n",
    "\n",
    "sns.lineplot(data=df_reddit_top_percent_trend, ax = ax1, palette = ['k','r','lime','b','cyan'], dashes=False)\n",
    "ax1.set_xticks(['2007','2009','2011','2013','2015','2017', '2019'])\n",
    "ax1.set_xlabel('Year', fontdict = {'fontsize': 12})\n",
    "ax1.set_ylabel('Percentage covered in Reddit', fontdict = {'fontsize': 12})\n",
    "ax1.set_title('A - Percentage of all attention received \\n by the most popular domains in Reddit' , \n",
    "              loc = 'left', fontdict = {'fontsize': 14, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True,  ax = ax1)\n",
    "legend1 = ax1.legend(prop={'size': 12})\n",
    "frame1 = legend1.get_frame()\n",
    "frame1.set_facecolor('white')\n",
    "frame1.set_edgecolor('white')\n",
    "\n",
    "\n",
    "sns.lineplot(data=df_twitter_top_percent_trend, ax = ax2, palette = ['k','r','lime','b','cyan'], dashes=False)\n",
    "ax2.set_xticks(['2011','2013','2015','2017', '2019'])\n",
    "ax2.set_xlabel('Year', fontdict = {'fontsize': 12})\n",
    "ax2.set_ylabel('Percentage covered in Twitter', fontdict = {'fontsize': 12})\n",
    "ax2.set_title('B - Percentage of all attention received \\n by the most popular domains in Twitter' , \n",
    "              loc = 'left', fontdict = {'fontsize': 14, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True,  ax = ax2)\n",
    "# legend2 = ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 10})\n",
    "legend2 = ax2.legend(loc='lower right', prop={'size': 12})\n",
    "frame2 = legend2.get_frame()\n",
    "frame2.set_facecolor('white')\n",
    "frame2.set_edgecolor('white')\n",
    "\n",
    "\n",
    "df_pr_trend.set_index('Year').plot(color = ['cyan','blue','lime'], ax = ax3)\n",
    "ax3.set_xlabel('Quarter', fontdict = {'fontsize': 12})\n",
    "ax3.set_ylabel('Percentage of links indexed across entire web', fontdict = {'fontsize': 12})\n",
    "ax3.set_title('C - Percentage of all attention received \\n most linked to domains across entire web' , \n",
    "              loc = 'left', fontdict = {'fontsize': 14, 'fontweight': 'bold', 'horizontalalignment': 'left'})\n",
    "sns.despine(offset=10, trim=True,  ax = ax3)\n",
    "legend3 = ax3.legend(frameon = 1, loc = 'center left', prop={'size': 12})\n",
    "frame3 = legend3.get_frame()\n",
    "frame3.set_facecolor('white')\n",
    "frame3.set_edgecolor('white')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
